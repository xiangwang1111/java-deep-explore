# 第一篇 开发基础篇

信息技术在过去的二三十年中发展非常迅猛，成为了许多人追逐的热门行业。但他们中有相当一部分既没有计算机专业科班的背景，也没有接受正规的、体系化的、循序渐进的理论指导和实践机会，即使是一些有着十多年工作经验的人也是如此。这就导致一个非常尴尬的现象：有的人可以把面试题倒背如流，也可以拿“高精尖”技术侃侃而谈，但在实际开发时却连最基本的技术概念都不甚了解，出现一些本不该有的低级错误。这就是本篇之所以会存在的主要原因之一——笔者希望通过自己有限的努力，把这个尴尬现象的影响面稍稍降低那么一点点。另一方面，笔者也希望借此机会对一些重要的开发常识和知识做些系统性的梳理、回顾和总结，抛砖引玉，以期让广大读者能够在信息技术行业，尤其是软件开发行业中走得更顺，更远。

“有了软肋，也就有了铠甲”——希冀这一篇的内容能襄助广大读者一臂之力，把“阿喀琉斯之踵”变为“海格力斯之箭”。

## 第1章 编程常识

常识很重要，这应该可以得到一部分人的认同。但哪些常识是日常开发工作中会高频接触到并需要了解的呢？

本章讲解了二进制、算法与数据结构、同步与异步、并发与并行、缓冲与缓存。它们虽仅占全部“常识域”的一小部分，且受篇幅所限无法深入展开，但就“二八定律”来说，用来厘清一些技术概念和解决实际问题已经够用了。掌握它们，对于开发者来说，有百利而无一害。

### 1.1 统治地球的冯·诺依曼们

1961年，一个叫叶永烈的21岁青年（他也是《十万个为什么》的作者，前网文时代的先驱），写了一本叫做《小灵通漫游未来》的科普小说。如图1-1所示。

> 图1-1 《小灵通漫游未来》

![图1-1 《小灵通漫游未来》](chapter01/01-01.png)

书中提到了可视电话、电子表、家庭机器人、助听器、隐形眼镜、人造食品、语音识别、远程教学等诸多“科学幻想”。时至今日，这些当年的“痴心妄想”早已成为现在人人都习以为常的东西。而这其中，计算机所起到的作用居功至伟。

世界上第一台“通用”计算机有个好听的名字：埃尼阿克（ENIAC，全称为Electronic Numerical Integrator And Computer，电子数字积分计算机）。严格来说，它不能叫做计算机，只能说是有一间房子那么大的计算器（而且还是世界上第二台计算器，不是第一台），如图1-2所示。

> 图1-2 世界上第一台“通用”计算机ENIAC

![图1-2 世界上第一台“通用”计算机ENIAC](chapter01/01-02.png)

因为ENIAC主要是由真空管拼凑起来的，里面包括了几百个电子逻辑门、开关和电线。所以也可以把ENIAC看成浑身长满大“灯泡”的铁箱子。这些构成逻辑门的真空管只有两种状态：不是被“打开”的状态，就是被“关闭”的状态（在经典物理学尚还存在情况下，不可能有第三种状态）。所以为了数学和物理表达上的简便，就用“1”表示“开”，用“0”表示“关”——计算机中的二进制由此诞生。

但在电子计算机诞生的一个多世纪以前（也就是1834年），就已经有人构思出了现代计算机的完整雏形——分析机。它拥有分工明确的处理器、控制器、存储器、输入与输出等不同装置。这是一个叫查尔斯·巴贝奇（Charles Babbage）的英国天才发明家的杰作。只是由于他的设计“过于先进”，那时候的世界还制造不出他所需要的设备。直到100多年后，才由一个叫约翰·冯·诺依曼的匈牙利裔美籍数学家、计算机科学家、物理学家、化学家、博弈论之父，跨越时空地实现了巴贝奇的天才构想，如图1-3所示。

> 图1-3 约翰·冯·诺依曼

![图1-3 约翰·冯·诺依曼](chapter01/01-03.png)

虽然关于谁才是真正的“计算机之父”至今没有确切的定论，有人认为是查尔斯·巴贝奇（通用计算机之父），有人认为是阿兰·图灵（Alan Turing，计算机科学之父），有人认为是约翰·阿坦那索夫（John Vincent Atanasoff，电子计算机之父），还有人认为是冯·诺依曼（现代计算机之父）。从不同的侧面来说，这都对。但是笔者认为，查尔斯·巴贝奇太过于超前，在错误的时间得到了正确的结果，抱憾终生；阿兰·图灵更侧重于密码学和人工智能在计算机上的应用；而约翰·阿坦那索夫虽然也摸到了现代计算机体系结构的大门，但终归还是一颗“近失弹”（Near Miss，军事术语，意思是说在极近的距离下失去目标，虽无杀伤但冲击力极大）。他们当中，只有冯·诺依曼是站在巨人的肩膀上（结合了包括莱布尼兹和查尔斯·巴贝奇等前人的科学理念），第一次完整地提出了现代计算机体系结构的基本思想。

在1944年，ENIAC还未建成之时，冯·诺依曼在返回洛斯·阿拉莫斯的列车上写出了那篇长达101页且影响整个计算机历史走向的《EDVAC报告书的第一份草案》，准备着手设计建造EDVAC（Electronic Discrete Variable Automatic Computer，电子离散变量自动计算机）。

这份草案不仅详细说明了EDVAC的设计思路，也指明了现代计算机（以下简称“冯·诺依曼机”）的发展道路：

1. 计算机需要使用二进制表示数据；
2. 计算机要像存储数据一样存储程序；
3. 计算机由运算器、控制器、存储器、输入和输出五大部分组成。

从那时起直到现在，不管是巨型机、大型机、中型机、小型机还是微型机（个人计算机）；不管是台式机、笔记本、PAD（平板电脑）、PDA（个人数字助理，在工业、医疗、物流等行业广泛使用，例如抄表器、扫码枪、护理机）、智能手机还是智能电视，全世界大部分的类计算机设备都是遵照冯·诺依曼当初所规定的体系结构设计并制造出来的。

当然，世界上除了冯·诺依曼机还有一些其他体系结构的计算机，例如光子计算机、分子计算机、量子计算机等。除非有特别说明，本书后续的内容都是建立在冯·诺依曼机的基础上进行论述的。

### 1.2 二进制的那些事

大多数人听到“二进制”的时候，脑海里可能马上就会联想到电影《黑客帝国》中由“0”和“1”组成的矩阵。

笔者不打算在这里详细讨论二进制的运算、反码、补码之类枯燥的东西，但有几个和开发相关的概念需要做一点澄清和普及。因为这些内容就像空气——用的时候不觉得，但一认真审视起来就容易犯迷糊。

#### 1.2.1 二进制文件

我们在计算机中看到的各种文件，例如文本、图片、音乐、视频、Word文件等，对于计算机来说，没有任何差别，因为它们都是由“0”和“1”组成的。但这么说还是太笼统，举个例子就很容易理解了。

首先，在Windows中下载并安装一个叫做Hex Editor Neo的软件，这是一种十六进制编辑器。当然也可以通过安装Vscode或Notepad++的插件的方式安装。

然后，在Windows的桌面上新建一个txt文本文件，名字可以任意起，在文件其中输入一些内容后保存，比如输入“Java编程语言”。

关闭文本编辑器窗口，然后光标悬停在文本文件的图标上并单击鼠标右键，选择用Hex Editor Neo软件打开它，如图1-4所示。

> 图1-4 用Hex Editor Neo打开文本文件

![图1-4 用Hex Editor Neo打开文本文件](chapter01/01-04.png)

打开后如图1-5所示。

> 图1-5 用Hex Editor Neo打开文本文件后显示的内容

![图1-5 用Hex Editor Neo打开文本文件后显示的内容](chapter01/01-05.png)

可以很清楚地看到，整个区域分为左中右三个部分。左边部分显示的是十六进制序号；中间部分显示的是刚才输入的内容，用十六进制数字表示；而右边则是内容的字符编码，只不过中文都变成了乱码。现在换成用二进制来显示它。依次点击菜单上的“View”->“Display As”->“Binary”。

切换之后，显示出来的二进制内容如图1-6所示。

> 图1-6 用二进制显示文本内容

![图1-6 用二进制显示文本内容](chapter01/01-06.png)

可以看到第一行第一列，原来显示十六进制的“4a”换成了二进制“01001010”。这正是“4a”对应的二进制数值，而将“01001010”转换成十进制数就是“74”。为什么要转换为十进制数呢？因为只有转换为十进制才能通过ASCII码表查到“74”所对应的字符是大写字母“J”，也就是刚才在文本中输入的第一个字母，如图1-7所示。

> 图1-7 十进制数字“74”对应字母“J”

![图1-7 十进制数字“74”对应字母“J”](chapter01/01-07.png)

依此类推，这些十六进制内容转换之后正是刚才输入的“Java编程语言”。

刚才展示的是文本数据，现在再来看一下图像数据。

用Photoshop或其他画图软件创建一个10×10像素的正方形，底色为白色，如图1-8所示。

> 图1-8 10×10像素的白色正方形

![图1-8 10×10像素的白色正方形](chapter01/01-08.png)

然后再次用Hex Editor Neo软件打开它，可以看到如图1-9所示的二进制内容。

> 图1-9 白色正方形图片的二进制内容

![图1-9 白色正方形图片的二进制内容](chapter01/01-09.png)

上图中间部分第一行第二至四列的内容分别为“01010000”、“01001110”和“01000111”，按照图1-7中的方法，它们分别对应ASCII中的“P”、“N”、“G”。

这正是文件名后缀“PNG”。这是巧合吗？并不是，因为在右边部分的剩余内容中可以看到其他和这个文件相关的一些信息，如创建文件的软件工具，文件创建时间等信息，这和用Windows属性工具显示出来的信息是一致的，如图1-10所示。

> 图1-10 图形文件的属性信息

![图1-10 图形文件的属性信息](chapter01/01-10.png)

这里没有再继续深入了解，但可以断定：Hex Editor Neo显示的内容一定包含了所有Windows属性显示的内容。

从这个意义上说，如果能够完全掌握用二进制创建文件的规则，是不是可以用Hex Editor Neo代替任何软件呢？例如用Hex Editor Neo代替文本编辑器，代替Word，代替Photoshop，甚至代替IDEA来编程呢？这不但理论上是完全可行的，而且事实上也确实可行。不过，却不会有人真的那么做，因为太费时费力，效率太低，而且极易出错。

我们平常所看到的任何文件，除了文件的内容本身，还含有一部分附加信息。这些附加信息用户是看不到的，即使看到了也没有意义。因为它们是给计算机操作系统准备的，用以区分各类不同的文件类型及读取、存储方式，如图1-11所示。

> 图1-11 操作系统读取二进制内容的方式

![图1-11 操作系统读取二进制内容的方式](chapter01/01-11.png)

从上图可以看出，操作系统和各种软件是这样工作的：

1. 读取时，操作系统通过附加信息就知道该将文件交给哪个软件处理、转译并展示；
2. 存储时，各种软件会先给文件添加专属的附加信息（软件安装时会在操作系统的注册表中“登记”这些附加信息），然后再交由操作系统一并保存；
3. 卸载后，由于对应的附加信息被从注册表中清除，所以操作系统也就不知道对应类型的文件该给哪种软件处理了。

这种附加信息有一个计算机专有名词：文件头。这也正是操作系统和各种应用软件存在的意义：有些文件头十分庞大，如果要人力用二进制的方式去编写完成，无疑既费力又不讨好，但计算机却十分擅长这种精确无误且枯燥无比的重复性劳动。

#### 1.2.2 字符集编码

由于计算机只能存储和处理二进制的“0”和“1”，无法处理其他的字母、数字和符号，所以就需要有某种东西来达到类似桥梁的作用——例如图1-7中的ASCII——通过它，人们就可以看懂用计算机表示字母、数字或其他符号。

人们能够想到的最直接的方法是就是对字母进行编号，例如A为1，B为2，C为3等。著名英国作家弗朗西斯·培根（Francis Bacon）曾用五位序列来编码英文的26个字母，在十六世纪用来传递密信。

如果以二进制来表示25（2的5次方，5位）也就是32，可以存32个字母，对于26个英文字母来说是足够用了。但它无法区分大小写字母，也无法再区分数字和标点符号。因此就有了ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）。

标准的ASCII码发明于1963年，但1967年才第一次发表，1986年则作了最后一次更新，目前ASCII包含27（2的7次方，也就是128）个字符。这个128个字符用来表示大小写字母、数字0～9、标点符号、以及像“#”、“@”、“）”、“\r”（回车）、“\n”换行、“\t”（制表）等这样的特殊符号，如图1-12所示。

> 图1-12 标准ASCII码表

![图1-12 标准ASCII码表](chapter01/01-12.png)

因为ASCII诞生的年代较早，所以使用非常广泛，使得不同的组织、不同的计算机之间能够互相交换数据与信息。但它也有个尴尬的限制：ASCII只为印欧语系日耳曼语族中的英语所设计，甚至都不兼容同一语系同一语族下的德语、荷语等其他语种。好在当初设计二进制的时候规定1个字节（Byte）由8位（Bit）二进制数组成，因此ASCII也就顺理成章地由7位扩展为8位（2的8次方），也就是扩展后ASCII有256个字符，又叫Extended ASCII（扩展ASCII，简称为EASCII）。

但EASCII由于国际化和标准化程度不够，所以就被设计更为优良的ISO/IEC 8859字符编码方案取代了。不过，即使是ISO/IEC 8859依然只是使用了单个字节，即8位来表示字符集，所以ASCII、EASCII和ISO/IEC 8859统称为单字节字符集（SBCS），而且ISO/IEC 8859也只能表示欧洲各国所使用到的字符，所以范围依然很有限。

当计算机在世界范围内的应用越来越广泛时，单字节字符集就已经完全无法使用了，因为仅仅中文常用汉字就有几千个，还不包括各种非常用字、生僻字。除此之外，每家计算机制造商、大的软件厂商也都发布自己的字符集。例如ASMO-708、DOS-720、Windows-1250、IBM EBCDIC等。这些字符集统称为多字节字符集（MBCS）。

为了解决这种“一锅粥”式的混乱局面，1992年，Unicode出现了。它为每种语言的每个字符设定了一个唯一的二进制编码，以满足跨语言跨平台的文本转换、处理要求。Unicode使用4字节共32位，也就是232个字符来填充字符集。Unicode本身只是一个字符集标准，针对它的实现称为UTF（Unicode Transformation Format，Unicode格式转换）。

顺便说一句，实际开发中使用UTF-8时有几个地方需要注意：

1. “UTF-8”是标准写法，因为Windows不区分大小写，所以写成“utf-8”也行。也可以把中间的“-”省略，写成“UTF8”；
2. 在MySQL的配置文件中只能使用“utf8”，如果写成“utf-8”或者“UTF-8”都不会生效；
3. 对于MySQL来说，“utf8mb4”才是真正意义上的“utf8”，因为低版本的MySQL中“utf8”最大字符长度为3字节，遇到表情之类的特殊字符就会出错。

由于Unicode的实现细节较为复杂，且了解这些对开发帮助不大，故无需深究。只需要知道：如果收发双方的计算机使用的都是Unicode编码，那么是绝不会出现乱码现象的。发出的内容是什么，收到的内容就是什么，也不会再出现类似“???”、“锟斤拷”、“�”等莫名其妙的字符了。

### 1.3 算法与数据结构

“算法”术语源于古代波斯博识者阿尔·花拉子密，也是1000多年前的代数之父。如图1-13所示。

> 图1-13 《代数学》创作者阿尔·花拉子密

![图1-13 《代数学》创作者阿尔·花拉子密](chapter01/01-13.png)

通俗地讲，算法就是完成计算的具体步骤（或办法）。同一种结果，往往可以用不同的算法写出来，而且有些算法会比其他算法更节省时间、节省空间。如何得到高效的算法，用尽可能少的步骤或时间解决问题，是早在现代计算机问世的千年之前就已存在的问题。

首先要澄清一下，当人们说到“算法”两个字的时候，可能不自觉地说的是它在广义上的概念，也就是既包括处理一般数据结构的基础算法，也包括用于处理大规模数据集的机器学习算法（或人工智能算法。严格来说机器学习和人工智能还是有差别的），如图1-14所示。

> 图1-14 算法在广义上的概念

![图1-14 算法在广义上的概念](chapter01/01-14.png)

但是笔者并不准备也不可能详细讨论所有的这两大类算法，这里只讲一讲基础算法。

算法既然决定处理数据的方法和效率，那么数据本身对算法有没有影响呢？或者说，计算机存储、组织数据的方式对算法有没有影响呢？——当然有，就像我们不太可能从字典的第一页去找单词一样。也正如收纳箱便于人们分类整理、存放衣服那样，结构清晰且分明的数据，也便于计算机的存储和处理。

#### 1.3.1 算法评价工具

在各种算法中，被用得最多的当推“查找”和“排序”这两类。这也是显而易见的——毕竟，人们在生活中就常常面临“找东西”和“比大小”的问题。比如，钥匙忘在哪儿了？忘单位还是忘家里了？如果是忘家里，是客厅还是卧室？——这就用到了简单的二分查找算法。或者，管他三七二十一，先从家里开始找，翻遍每一个角落，不行再去单位翻遍每一个角落——这又用到了顺序查找。再比如，如果用字典查英语单词“Java”的意思，人们一般会从“J”开头的字母开始往后翻，而既不是从头开始，也不是从字典的中间开始，这也是一种查找算法，称为索引。“比大小”也是一样：“高矮胖瘦”和“多快好省”，无一不是排序在起作用。

计算机科学研究算法的目的就是要提高计算效率，即用尽量少的时间和空间，在有限资源下求最优解。迄今为止，各类计算机算法已有成百上千种，即使是像排序这样“简单”的算法也有十好几种。那怎么衡量哪种算法更高效，也就是更省时间或者更省空间呢？因此，科学家们提出用“大O表示法”（Big-O）来衡量算法的执行效率。其中：

1. 衡量算法执行时间效率的称为“时间复杂度”，定义为T(n) = O(f(n))；
2. 衡量算法执行空间效率的称为“空间复杂度”，定义为S(n) = O(f(n))。

所谓算法的时间复杂度，指的是某种算法运行一次所需要的时间。但这个时间不是具体的时、分、秒，而是执行运算的操作次数——很显然，操作次数越少，需要的时间也就越少。例如，如果在100个数中用顺序查找的方法找到“1”这个数字，那么在最坏的情况下（从第一个数开始，找到最后一个数），需要“找”100次，也就是要执行100次比较操作。所以它的时间复杂度为O(100)，这是一种非常简单的线性复杂度。

有的算法不管有多少需要查找的数据，它的复杂度始终都不变，只有1次，或至多2～3次。那么它的复杂度可以就用O(1)来表示。例如查找算法中的索引查找或哈希查找。还有的算法的时间复杂度是以某数为底的对数，例如二分查找就是以2为底的对数log，因为它每次都只对一半的数值进行查找。为什么是对数呢？最直观的解释就是图1-15所示那样。

> 图1-15 算法复杂度O(log n)的直观解释

![图1-15 算法复杂度O(log n)的直观解释](chapter01/01-15.png)

所以，按照时间复杂度，各种查找算法的时间复杂度排序如表1-1所列的那样。

> 表1-1 几大常见查找算法的时间复杂度

| 算法名称 | 算法复杂度 |
|:---:|:---:|
| 顺序查找 | O(n) |
| 二分查找（折半查找） | O(log n) |
| 插值查找 | O(log log n) |
| 哈希查找 | O(1) |

除了这几种常见的算法及其复杂度之外，还有很多其他的算法及其复杂度。笔者就不一一说明。图1-16展示了一个算法时间复杂度的比较，它们与算法类别无关，适用于查找、排序及其他算法。

> 图1-16 算法时间复杂度的比较

![图1-16 算法时间复杂度的比较](chapter01/01-16.png)

按照时间复杂度从小到大的排序，它们分别是：O(1) < O(logn) < O(n) < O(nlogn) < O(n2) < O(n3) < 0(2n) < O(n!) < O(nn)。

在算法执行时，通常需要用到三种不同类型的存储空间：

1. 输入空间：保存输入数据所需存储空间的大小；
2. 暂存空间：算法执行时，存储中间变量等数据所需的空间大小；
3. 输出空间：保存输出数据所需存储空间的大小。

一般情况下，算法的空间复杂度 = 暂存空间 + 输出空间大小。

和算法时间复杂度一样，常见的空间复杂度及其排序是：O(1) < O(logn) < O(n) < O(n2) < 0(2n)。

如图1-17所示。

> 图1-17 算法的空间复杂度

![图1-17 算法的空间复杂度](chapter01/01-17.png)

所以，当需要通过算法执行某些具体任务时，只需要通过本书给出的时间复杂度和空间复杂度的总结和排序，也就是算法的通用工具，就能大概知道该选取哪种算法比较合适了。当然，也可以求助一些在线的算法复杂度计算工具，看看算法是否能达到预期要求。

#### 1.3.2 图解数据结构

现在就来捋一捋前面提到过的那些数据结构，也顺便澄清一些数据结构中让人常“犯迷糊”的地方。

首先是“数组”。有的编程语言称为Array，但在Java中，ArrayList的底层就是由数组支撑的。它的特点如图1-18所示。

> 图1-18 数组

![图1-18 数组](chapter01/01-18.png)

1. 数组只能存储相同类型的元素，也就是说要么是数字，要么是字符，要么是对象；
2. 数组长度在初始化分配时就固定下来了；
3. 数组元素是顺序且连续存放的，所以元素地址也是顺序且连续的；
4. 数组可以随机访问。

数组读取效率高，但插入、删除很麻烦。

“链表”则是一种特殊的List列表，如图1-19所示。

> 图1-19 单向链表、循环链表与双向循环链表

![图1-19 单向链表、循环链表与双向循环链表](chapter01/01-19.png)

相较于数组，它有如下特性：

1. 和数组一样，链表也只能存储相同类型的元素；
2. 链表长度在实际使用时是可变的；
3. 元素的存储地址不是连续的内存空间；
4. 链表是有方向的，有空链表、单向链表、双向链表和循环链表；
5. 链表只能按序遍历；
6. 插入、删除效率高，但读取比较麻烦。

上图做了简化，没有给链表加上头尾指针或者所谓的Guard位。所以稍作总结，数组和链表的区别如表1-2所示。

> 表1-2 数组和链表的区别

|  | 数组 | 链表 |
|:---:|:---:|:---:|
| 内存地址 | 连续的内存空间 | 不连续的内存空间 |
| 长度 | 分配时固定 | 分配时可指定，但也能动态调整 |
| 访问效率 | 增删效率低，读取高 | 增删效率高，读取低 |
| 访问方式 | 可以随机遍历 | 只能顺序遍历 |
| 方向 | 无 | 有 |

“栈”如图1-20所示，用一句简单的话来说就是“先进后出”（FILO，First In Last Out）。

> 图1-20 栈

![图1-20 栈](chapter01/01-20.png)

1. 栈是内存中的一块区域，是一个先进后出的线性表；
2. 用于存储运算时需要的临时数据，大小固定，由程序自动创建和释放；
3. 容量较小，一般只有几KB～几MB大小，但访问速度很快。

“队列”和栈类似，它和栈唯一的区别是“先进先出”（FIFO，First In First Out）。

1. 只能在队头端插入，而在队尾端删除；
2. 链式队列长度不固定，插入删除方便，读取不便。如图1-21所示；

> 图1-21 普通链式队列

![图1-21 普通链式队列](chapter01/01-21.png)

3. 循环队列长度固定，插入删除不便，读取方便。如图1-22所示。

> 图1-22 循环队列

![图1-22 循环队列](chapter01/01-22.png)

“树”作为数据结构中的一个大类，在底层中间件中的应用非常广泛。例如MySQL数据库的索引就是用B+树实现的。在搞清楚一系列各种不同的树之前，要先知道关于树都有哪些名词、术语和属性，不然有些树的定义或区别无法描述。如图1-23所示。

> 图1-23 关于“树”的各种属性

![图1-23 关于“树”的各种属性](chapter01/01-23.png)

1. 节点：组成树的基础，它有自己的名字或“键”，节点可以带有附加信息，称为“有效载荷”。上图中A、B、C、D、E、F、G、H都是树的节点；
2. 边：组成树的基础，节点之间通过边相连。箭头指向节点的边称为入边，反之为出边，图中红色的边L就是A的出边，B的入边；
3. 根节点：处于顶层且没有入边只有出边的节点。A就是整棵树的根节点；
4. 父节点：有出边指向其他节点的节点。A就是B的父节点；
5. 子节点：有入边且和上层节点相连的节点。B就是A的子节点；
6. 内部节点：除根节点和叶子节点之外的节点叫做内部节点。B和C都是内部节点，它们既非根节点也非叶子节点；
7. 叶子节点：只有入边没有出边的子节点。D、E、F、G、H都是叶子节点，它们都只有入边没有出边；
8. 度：每个节点的子节点或叶子节点的数量，称为degree。A节点和B节点的度degree都是2。C节点的度degree是3。而D、E、F、G、H这几个叶子节点的度degree是0；
9. 阶：直观且简单地说，度数值最大的子节点即为该树的阶，称为order。阶是针对整棵树而言的，因为C节点的度degree最大为3，所以上图中整颗树的阶order就是3；
10. 树的深度：从根节点开始计数，如根节点可以记为为0或1，自顶向下逐层累加后的值即为树的深度。步长可为1或更大的数；
11. 树的高度：从叶子节点开始计数，如叶子节点可以记为为0或1，自底向上逐层累加后的值即为树的高度。步长可为1或更大的数；
12. 深度和高度可以是整棵树的，也可以是某个节点n[i]的。如果是某个节点n[i]的深度，则是从根节点开始到节点n[i]之间的路径长度；如果是某个节点n[i]的高度，则是从叶子节点开始到n[i]节点之间的路径长度。

在厘清了树的定义之后，就可以再来看看树有哪些不同的分类了，如图1-24所示。

> 图1-24 普通的树、二叉树、满二叉树和完全二叉树

![图1-24 普通的树、二叉树、满二叉树和完全二叉树](chapter01/01-24.png)

1. 普通的树：每个节点有0个或者多个子节点；
2. 二叉树：每个节点有0个或者最多2个子节点；
3. 满二叉树：它是一颗二叉树，且除叶子节点外，其他各层的节点都有且仅有2个子节点；
4. 完全二叉树：它是一颗二叉树，且从根节点至倒数第2层是一颗满二叉树，叶子节点会优填充左子树。
5. 二叉查找树（BST）：又称二叉搜索树，它是一颗二叉树，如图1-25所示。
  - 若其左子树不空，则左子树上节点的值必定小于它根节点的值；
  - 若其右子树不空，则右子树上节点的值必定大于它根节点的值；
  - 任意节点的左右子树也都是二叉查找树。
6. 平衡二叉树（AVL）：它是一颗二叉查找树，其左右子树的高度差的绝对值不超过1，且左右子树也都是平衡二叉树，如图1-25所示。
7. 红黑树（RBT）：它是一颗二叉查找树，如图1-25所示。
  - 根节点和叶子节点都是黑色的，且叶子节点都是值为null的节点；
  - 每个节点要么是红色的，要么是黑色的；
  - 红色节点的子节点和父节点都是黑色的；
  - 从任一节点到叶子节点，其路径上包含的黑色节点数量相同。
8. “B树”：有些文档或资料中也叫它“B-树”，但称它为B-树有些不妥，因为称其为B-树，可能会让人误认为B-树是另一种数据结构。所以没有所谓的B-树，只有B树。如图1-26所示。

> 图1-25 二叉查找树、平衡二叉树和红黑树

![图1-25 二叉查找树、平衡二叉树和红黑树](chapter01/01-25.png)

> 图1-26 B树的结构

![图1-26 B树的结构](chapter01/01-26.png)

一个m阶的B树具有如下定义：

- 根节点不是叶子节点时，根节点有n个子节点，2 <= n <= m；
- 每个内部节点都至少有n个子节点，n = Math.ceil(m / 2)。且其所含键k的数量范围为n - 1 <= k <= m - 1，键是用于指向数据记录的指针。Math.ceil()函数表示向上取整，例如Math.ceil(1.1)和Math.ceil(1.8)向上取整的结果都为2；
- 有n个键的非叶子节点必须有n + 1个子节点；
- 所有叶子节点的深度（或高度）都一样，也就是说它们都要在同一层；
- 每个节点中键的数值都从小到大排序，且每个节点的子树也按照下列顺序从左至右依次排列：
  - 第一个子树的所有键值都小于其父节点的最小键值；
  - 第二个子树的所有键值都大于其父节点的最小键值，而又小于其父节点的第二小键值；
  - 其他中间子树的键值属性及排列位置依此类推；
  - 最后一个子树的所有键值都大于其父节点的最大键值。

从上图也可以很容易地看出键值排列的规律：

- 左节点B中的所有键值都要小于父节点A中最小的键值“8”；
- 中间节点C中所有键值都要大于父节点A的最小键值“8”，但又都小于父节点A第二小的键值“11”；
- 右节点D中的所有键值都要大于父节点A中最大的键值“11”。

下图1-27和图1-28展示了B树从1到14的插值生成过程。如果阶数太少说明不了问题，但阶数多了过程又太冗长。以4阶B树作为演示刚好合适。在4阶B树中，每个结点最多只4个子节点，最多只有3个键（key = m阶 - 1），最少有1个键（key = Math.ceil(m / 2) - 1），有n个键的非叶子节点必须有n + 1个子节点。所以整个插值过程如下。

> 图1-27 B树的生成过程：第1步至第5步

![图1-27 B树的生成过程：第1步至第5步](chapter01/01-27.png)

> 图1-28 B树的生成过程：第6步至第8步

![图1-28 B树的生成过程：第6步至第8步](chapter01/01-28.png)

从图1-28生成的结果可以看到它和图1-26有些不同。这也是笔者想指出的B树的另外一个“隐藏”属性：

- 如果不是从1开始插值，而是从其他数开始，也就是说生成B树的插值顺序不同，那么生成的B树也就不同；
- 如果第2步选择键值“2”作父节点，那么最终生成B树的结果也会不同。

以上两点充分说明：插值顺序和权重对生成B树的结果有直接影响。这也是很多资料当中都没有提到的一点。

9. B+树：和B树类似，它在B树的基础上做了一些改变，如图1-29所示。

> 图1-29 B+树

![图1-29 B+树](chapter01/01-29.png)

B+树的生成和B树非常类似，只是它所有的数据都在叶子节点上而已，此处就不再展示其生成过程了。

在树型数据结构中，B树和B+树比较复杂，如果是需要开发文件系统一类的应用，或者数据库底层存储，那就必须要深入掌握它所涉及到的每一个技术细节。否则，了解其原理就行了。但MySQL的底层存储是通过B+树实现的，所以有些面试官比较爱问和B+树相关的问题。

知道了树再来理解“堆”就不难了，因为它几乎总是一颗完全二叉树，如图1-30所示。

> 图1-30 最大堆和最小堆

![图1-30 最大堆和最小堆](chapter01/01-30.png)

“堆”具有如下特性：

1. 从根节点开始，堆的序号是从上至下，从左至右；
2. 可以直接用数组来表示一个堆，因为对任意一个父节点n，它的子节点的序号一定是2n+1和2n+2；
3. 对于最大堆来说，其父节点的键值比其子节点的键值都大；
4. 对于最小堆来说，其父节点的键值比其子节点的键值都小。

堆栈常被并列提及，是因为它们在开发时所起的作用较大，尤其是在JVM这一块，表1-3将它们作了一个简单比较。

> 表1-3 栈和堆的区别

|  | 栈 | 堆 |
|:---:|:---:|:---:|
| 分配方式 | 程序自动分配，也可用JVM设置 | 程序自动分配，也可用JVM设置 |
| 内存大小 | 很小，一般几KB～几MB | 较大，可达到几个GB，甚至TB |
| 访问速度 | 较快 | 比栈要慢 |
| 应用场景 | 保存线程临时变量、对象 | 保存一些全局、静态的变量和对象 |

“散列表”（又叫哈希表，Hash Table）是一种通过某种函数，将“键”映射为“值”的数据结构。它的算法的时间复杂度是恒定的O(1)，如图1-31所示。这个映射函数也称为哈希函数。

> 图1-31 哈希函数

![图1-31 哈希函数](chapter01/01-31.png)

“图”的数据结构虽然在理论学习的资料中出现不多，但其实很多人每天都在用：地图导航，如图1-32所示。

> 图1-32 有向图和无向图及其邻接矩阵

![图1-32 有向图和无向图及其邻接矩阵](chapter01/01-32.png)

图具有如下属性：

1. 图包括顶点和边，边可以根据顶点之间的关系设置不同的权重；
2. 图分为有向图和无向图；入度：有向图的某个顶点作为终点的次数和；出度：有向图的某个顶点作为起点的次数和；
3. 图可以用邻接矩阵表示，也就是通过二维数组表示顶点间是否相连，或者表示顶点之间的权重；邻接矩阵的行或列都可以代表起点或终点；
4. 有向图上从起点A到终点B之间无连接，则邻接矩阵上的值为0，从起点B到终点A之间有连接，则邻接矩阵上的值为1；
5. 无向图的邻接矩阵基于主对角线对称，如图1-33所示。有向图无对称属性；
6. 图也可以用邻接表来表示，在邻接表中，每一个顶点都是一个链表的表头，链表的节点则是与该顶点有连线的顶点；
7. 邻接表和逆邻接表组合在一起就是十字链表。

> 图1-33 邻接表

![图1-33 邻接表](chapter01/01-33.png)

由于逆邻接表、十字链表在本质上和邻接表类似，所以这里就不做重复讲解了。

最后是“串”，这是一种比较特殊的数据结构，它对应到Java编程语言就是字符串类型String。它是由0个或者多个字符组成的有限序列，一般记为：S =“a<sub>1</sub>a<sub>2</sub>...a<sub>n</sub>”(n >= 0)。其中：

1. S为串名，单引号括起来的字符序列是串的值；
2. a<sub>i</sub>表示字符，可以是字母、数字或特殊字符；
3. 串中字符的个数n称为串的长度；
4. n=0的串称为空串（用∅表示）；
5. 字符串可以用定长数组、堆和块链结构来存储。如图1-34所示。

> 图1-34 串的三种存储方式

![图1-34 串的三种存储方式](chapter01/01-34.png)

数据结构作为整个计算机科学大厦的奠基石之一，博大精深，在和计算机相关的所有领域几乎都能看到它的身影。而且，它本身还在不断地演化发展着，即便是是简单如数组、栈、链表等数据结构，在从事具体工作时都会有很多可以优化的地方及使用技巧。只有将它们灵活地应用于实际开发中，才可以说是真正意义上的熟悉和精通。

### 1.4 同步与异步

在日常生活中，有些事情需要一件件地做，只有做完前面一件以后才能接着做后面一件。比如当我们逛完街下馆子吃饭的时候，遇到人多就只能先排队才能就餐；或者家里没酱油了，就需要先去打了酱油回来才能炒菜；再或者遇到那种“在线等，挺急的”问题。这些情况或事件，都是按照事情发展的逻辑顺序来执行的，只有当前面一步有了结果之后，后一步才能继续进行。

但是，如果不改变事情的执行步骤，而是通过一些手段，让这些本来只能以先后顺序依次执行的任务，可以“同时”进行，体验是不是会更好一些呢？以上发生在生活中的种种，在计算机中同样也存在。这种要等待上一步操作返回结果才能进行下一步的方式，在计算机科学中有一个专门的术语，称为“同步”；而不必等待上一步的结果就可以直接进行下一步的操作方式，则称之为“异步”，如图1-35和图1-36所示。

> 图1-35 “同步”排队

![图1-35 “同步”排队](chapter01/01-35.png)

> 图1-36 “异步”排队

![图1-36 “异步”排队](chapter01/01-36.png)

从图中可以看到，通过异步执行方式，客人留下联系方式之后就可以继续去干别的事情了。而当有空位时，餐馆只需要给客人发送通知即可。这样，既不用客人焦急干等，餐馆也不会因此而失去一位潜在的顾客。这种“异步”的交易方式使得双方的体验都有了本质的改善。

在计算机科学中，对这两种不同操作方式的定义为：

1. 同步：当出现多个有序的事件、操作或进程时，只有按照这种已设定的次序将某一个事件、操作或进程执行完成并返回结果之后，紧邻其后的下一个事件、操作或进程才能继续开始执行，如图1-37所示。

> 图1-37 同步执行任务

![图1-37 同步执行任务](chapter01/01-37.png)

2. 异步：当出现多个有序的事件、操作或进程时，虽然在整体上依旧按照这种已设定的次序，但却在某些事件、操作或进程执行完成返回结果之前，就接着执行紧邻其后的下一个事件、操作或进程，如图1-38所示。

> 图1-38 异步执行任务

![图1-38 异步执行任务](chapter01/01-38.png)

作为工程师文化的一部分，有一句名言流传甚广：“Talk is cheap, show me the code”。现在就用代码来看看同步和异步的区别。
拿全职太太姬如雪来举例。她每天都需要做的几件事就是洗衣、遛娃等。尤其是洗衣服，如果用手洗的话比较累也比较费事，如代码清单1-1所示。

代码清单1-1：Jiruxue.java

```java
public class Jiruxue {
    public void washClothes() {
        try {
            System.out.println("手洗衣服1个小时");
            TimeUnit.MILLISECONDS.sleep(3000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println("衣服洗完了");
    }

    public void dosomething() {
        System.out.println("逛街、买菜、遛娃 + 遛狗中......");
    }

    public static void main(String[] args) {
        System.out.println("开始洗衣服");
        Jiruxue jiruxue = new Jiruxue();
        jiruxue.washClothes();
        jiruxue.dosomething();
    }
}
```

每次洗衣服她都要花很长时间，而且也干不了别的。所以，如果能够“异步”地洗衣服该有多好。

异步在软件开发中只是一种思想，要把这种思想落实到实际可操作的层次，就需要借助另一种东西：回调接口。所谓回调接口，其实就是一种通知机制：当客人给餐馆留下自己的电话号码之后，如果出现空位，那么餐馆老板就可以通过这个电话号码通知客人来就餐了。所以“回调接口”的直白解释就是“反过来调用电话接口通知客人”。

现在，家里购置了洗衣机，就不用姬如雪自己洗了。只需要衣服洗好之后等待洗衣机的“通知”，拿出来晾晒即可，如图1-39所示。

> 图1-39 姬如雪“异步”洗衣服

![图1-39 姬如雪“异步”洗衣服](chapter01/01-39.png)

这里之所以没有“登记”这一步，是因为洗衣机发出的声音会很自然地被耳朵听到（这里不考虑远程APP等方式），这已经就是“登记”了。

用回调接口的方式来实现“异步”洗衣的代码如代码清单1-2所示。

代码清单1-2：HostessJiruxue.java

```java
@FunctionalInterface
interface Wash {
    public void finish();
}

public class HostessJiruxue {
    public void washClothes(Wash wash) {
        try {
            TimeUnit.MILLISECONDS.sleep(3000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        wash.finish();
    }

    public void dosomething() {
        System.out.println("逛街、遛娃 + 遛狗中......");
    }

    public static void main(String[] args) {
        System.out.println("把衣服放到洗衣机，先忙别的");
        new Thread(
                () -> new HostessYunxi().washClothes(
                        () -> System.out.println("嘀～嘀～嘀～.......（衣服洗完了）")
                )
        ).start();
        new HostessYunxi().dosomething();
    }
}
```

执行后看到，女主人姬如雪没有等衣服洗完就直接去做其他家务了。为了精简代码，这里用了一个函数式接口Wash（后面会单独讲到）来实现回调。

以上就是同步、异步与回调这几个概念的说明。另外，有几个概念需要一并澄清：

1. 同步不完全等于阻塞，异步也不完全等于非阻塞。同步和异步指的是一种思想，是指有先后次序或者因果关系的任务的执行方式，且异步还可能涉及到回调操作。而阻塞和非阻塞一般指的是多线程的运行状态，是多个线程在运行时对资源的争夺：拿到资源的线程继续执行，没拿到资源的线程则会被“阻塞”。不过在一般情况下，“同步和阻塞”、“异步和非阻塞”的概念是可以互换的；

2. 异步也不等于并发（就更谈不上并行了，这是两个完全不同的概念）。用异步方式执行的前后两个事件、任务或进程之间往往有先后次序或者因果关系，而并发则没有，并发关注的仅仅是CPU资源，如并发传输、并发抢票等；

3. 常说的“数据同步”中的“同步”和这里说的“同步”也不是一码事。数据同步指的是保持不同的文件、信息、数据表在不同存储介质中的内容的一致性。这里的同步指的是一组有先后次序或者因果关系的任务之间的执行方式；

4. 可以说，弄明白了“同步和异步”、“阻塞和非阻塞”、“并发和并行”这几个概念及其之间的区别、联系，也就真正弄明白了计算机程序运行的本质。

### 1.5 并发与并行

在学习Java或其他编程语言时，会经常碰到“并发”和“并行”这两个名词术语。它们是计算机科学领域中非常重要的两个概念，在系统性能优化和资源利用方面有着无可替代的作用。它们看起来好像差不多，但区别还挺大的。因为这两个概念都涉及到计算机上最重要的部件之一——CPU的优化和利用。按照定义：

1. 并发（Concurrency）是指在同一个时间段内，单个CPU在多个任务间交替执行。因为在多任务操作系统中，不同任务是可以共享同一个CPU资源的，每个任务都会分配到一定的时间片来执行计算，直到该任务完成或者分配的时间片用完为止。因此，虽然它们在微观上是交替执行，但在宏观上看起来就好像是同时在执行一样；

2. 并行（Parallelism）是指在同一个时间段内，多个CPU中的每一个都可以同时执行不同的任务，或者同时执行同一个任务中的不同子任务。所以，不但在宏观上表现出来的是同时执行的状态，在微观上也是真正的在同时执行不同的任务。通过这种方式提高系统的性能和效率。

上面这两段话可能看起来不太好懂，不过通过图1-40和图1-41就应该能明白了。

> 图1-40 “并发”的中二学生

![图1-40 “并发”的中二学生](chapter01/01-40.png)

> 图1-41 并行的闸机通道

![图1-41 并行的闸机通道](chapter01/01-41.png)

图1-40是一张很普通的八年级中二课程表。从并发的角度来理解，同样一个中学生，一天内可以通过预先分配的不同课时（时间片），交替学完六节不同的课。宏观上一天内同时学了六门课，但在微观上，是在不同的课时内完成的。

而图1-41则是非常普通的闸机通道。可以同时通过（执行）很多乘客（任务）。这是一种真正的同时执行，不光宏观上，微观上亦是如此。如果不开辟多个通道，那么可能就会有很多乘客被卡在大门口，赶不上地铁、火车或者飞机，这正是并行的意义所在。

具体到计算机来说，单个CPU多个任务的并发过程如图1-42所示。

> 图1-42 计算机领域中的并发

![图1-42 计算机领域中的并发](chapter01/01-42.png)

上图中虽然有三个不同的任务正在执行，但在每一个时间片内，只有一个小任务可以分配到CPU的资源执行计算。而图1-43展示了多CPU执行多任务的并行过程。

> 图1-43 计算机领域中的并行

![图1-43 计算机领域中的并行](chapter01/01-43.png)

通过这种对比，就能清楚地了解到CPU并行执行的好处。如果把并发与并行这两种技术集合在一起，如图1-44所示。

> 图1-44 并发与并行共同作用

![图1-44 并发与并行共同作用](chapter01/01-44.png)

在单个CPU内是并发，而在多个CPU的范畴内又是并行，将计算机的整体系统性能和资源利用率优化到极致。

### 1.6 缓冲与缓存

由于CPU的运算速度极快，而一些硬件设备没有那么快的速度，例如硬盘、网卡、键盘、鼠标等，因此它们之间的速度无法匹配。如果执行任务时CPU总是需要不停地中断自身操作来“等待”那些慢速设备，无疑既会给计算效率造成极大的损害，又会严重违背CPU更高速度、更高性能的设计初衷，这显然是不行的。

最终，科学家们解决这个问题的思路是通过引入一种叫做“高速缓存”（Cache）的中间部件，让（设备的）慢“跟上”（CPU的）快，如图1-45所示。

> 图1-45 带高速缓存的计算机体系结构

![图1-45 带高速缓存的计算机体系结构](chapter01/01-45.png)

从图中看到，在CPU里边有一个专为解决速度不匹配问题而存在的高速缓存，在它里面存放的都是CPU即将执行的任务所需用到的数据。这个高速缓存还有另一个名称，叫做“缓冲”（Buffer）。有了它之后：

1. CPU得以解除所受到的速度制约，可以更专注于执行计算任务；
2. 提高了计算效率。如果每次CPU都直接从RAM存储器获取计算数据，那将频繁出现卡顿现象，性能会非常低下。

所以，相对于CPU而言，高速缓存起到了“缓冲”的作用。而相对于计算机磁盘而言，RAM缓存也起到了“缓冲”的作用。因此，“缓冲”Buffer与“缓存”Cache实际上指的并不是两种具体的计算机设备，而是两种对数据的不同处理方式，如图1-46所示。

> 图1-46 缓冲Buffer和缓存Cache的区别

![图1-46 缓冲Buffer和缓存Cache的区别](chapter01/01-46.png)

如上图所示，缓冲Buffer和缓存Cache的区别，就像我们每个人家里冰箱和砧板/灶台的区别——因为即使没有冰箱和砧板/灶台，也一定会有其他类似的东西来补位。所以，缓冲和缓存的区别如表1-4所示。

> 表1-4 缓冲和缓存的主要区别

|  | 缓冲Buffer | 缓存Cache |
|:---:|:---:|:---:|
| 存储空间大小 | 很小，只有KB级 | 很大，可以达到GB甚至TB级 |
| 生命周期 | 毫秒甚至纳秒级 | 秒级，甚至分钟、小时级 |
| 使用场景 | 即将参与运算的数据 | 临时变量、对象及静态类等 |
| 存储介质 | CPU寄存器 | RAM |

既然提到缓存，就稍微扩展讲一点开发中经常看到所谓“本地缓存”、“分布式缓存”、“进程内缓存”和“进程外缓存”的概念。

#### 1.6.1 本地缓存与分布式缓存

软件开发中所说的缓存Cache并非指的计算机中的RAM，而是另有所指。

随着业务规模的不断扩大，传统的单体软件架构也逐步升级成了分布式系统，之前那种只存在于某台计算机之内用于存储临时数据的组件，相对于当前的计算机来说，就称之为“本地缓存”。本地缓存可以是一个Java的集合类，例如List、Map或Set；也可以是Python中的散列表，例如Tuple、Set或Dict；也可以是第三方组件，例如Google Guava、Ehcache或Caffeine，或者兼而有之。总之，它只能缓存单个节点的业务系统内的数据，如图1-47所示。

> 图1-47 本地缓存

![图1-47 本地缓存](chapter01/01-47.png)

与本地缓存相对的是分布式系统中的缓存。它为多台计算机，也就是多个业务节点提供服务，可以满足保存同一种类型业务数据的需求。例如分布式系统中的用户点赞数据，可能就是由诸如Redis、Memcached或Cassandra等NoSQL缓存中间件保存的，也有可能是由诸如MongoDB、Elasticsearch之类的索引型文档库保存的，甚至也有可能是由Nginx集群保存的。不管哪一种，它们都可以为分布式系统提供一个数据访问接口，保证用户相关数据始终只有唯一的一。如图1-48所示。

> 图1-48 分布式缓存

![图1-48 分布式缓存](chapter01/01-48.png)

#### 1.6.2 进程内缓存与进程外缓存

缓存还有另外一种分类方式，就是进程内缓存与进程外缓存。所谓进程，指的就是某个应用程序运行的时候所对应的计算机进程，而“进程内”具体就是编程语言所提供的类似于散列表的组件，例如Java的集合，Python的元组或字典等。因为它们是直接运行在应用程序的进程之内的。

相对于进程内缓存，就肯定会有进程外缓存了。除进程内以外的缓存，可以统统称为进程外缓存。像之前提过的Caffeine、Redis、Elasticsearch或Nginx等，都属于此类。

在有些技术文档中，会把本地缓存和分布式缓存，进程内缓存和进程外缓存这几种混为一谈。不过只要清楚开发的上下文环境和它的作用，就算是弄混也没什么问题。

### 1.7 本章小结

本章首先谈到了冯·诺依曼架构，它给现代计算机科学的发展方向造成了重大影响，也造成了如今整个计算机产业蓬勃发展的局面。

二进制不仅是一种计数方法论，也是整个计算机科学能够存在的基石。在计算机“眼里”，从来就没有什么文本、图片、音乐、视频、doc、xlsx、pdf、网页等我们所认为的一大堆东西，计算机只懂也只可能懂两个东西：数字“0”和“1”。它们不仅创造了人们肉眼可见的数千万种色彩，也统一了全世界的文本字符表示方法。

算法和数据结构是计算机科学中核心内容之一，它们随着计算机编程语言的诞生而诞生，虽名字不同但都有着同样的目的：以尽可能少的时间和尽可能少的空间完成计算任务。只不过算法是专注于计算的方法和步骤上，而数据结构则是专注于待计算数据本身的结构上。

除了算法和数据结构，为了进一步提升计算机的执行效率，科学家们将“发明”了同步和异步两种工作模式。同步模式需要等待前一步的返回结果才能执行下一步计算，所以它只能以串行的方式一件事接一件事地做；而异步模式则不需要，它可以暂时忽略返回值而先执行其他的任务。异步模式的实现手段就是回调接口。同步和异步、阻塞和非阻塞在大部分语境下都是可以互相替换的。

所谓并发，就是让单个CPU“同时”执行多个计算任务。这种“同时”，在宏观上来看好像确实是“同时”，但在人无法察觉的微观层面，CPU的时间片是以毫秒为单位的，所以本质上还是一种串行。而“并行”则是指多个CPU真正地同时执行多个计算任务。

为了匹配CPU的高速和周边设备的低速，科学家引入了高速缓存Cache这样的设备。相对于RAM，高速缓存更快，所以它又有一个不同的名字：缓冲Buffer。缓冲和缓存的区别，就好比我们每个人家中砧板和冰箱的区别。本地缓存和分布式缓存、进程内缓存和进程外缓存这些概念在有些情况下就算是弄混也没什么问题。

笔者希望这些常识性的内容，能够巩固与加深读者对计算机软件开发的整体认识，并成为今后职业生涯中向上攀登的坚实基础。

### 1.8 本章习题

1. 在第1.2.1小节中的表1-1只给出了部分查找算法的时间复杂度。读者可以试着完善这个表，列出其他查找算法的时间复杂度。然后再进一步继续列出常见排序算法的时间复杂度，作为自己的私人宝藏。如果有的读者觉得比较轻松，那么还可以继续向上攀登：列出常见查找和排序算法的空间复杂度。

2. 读者可以尝试画出图1-17所对应的逆邻接表。然后尝试列出各类常见数据结构的使用场景。

3. 参照女主人姬如雪洗衣服的代码，尝试用回调接口实现图1-20的案例。

4. 在Windows系统中，可以在命令行中运行“Resmon”命令查看各个CPU的并发和并行状况，而MacOS则可以通过活动监视器查看。如图1-49和图1-50所示。读者可以参照它们看看自己的计算机CPU的数量、并发和并行状况。

> 图1-49 Windows上的CPU并行计算

![图1-49 Windows上的CPU并行计算](chapter01/01-49.png)

> 图1-50 MacOS上的CPU并行计算

![图1-50 MacOS上的CPU并行计算](chapter01/01-50.png)
